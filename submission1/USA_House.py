# -*- coding: utf-8 -*-
"""Copy of diamons.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VqEZOzD2qtRIQUjPMMYWfYGwiTLyytvs

##Import liblary yang di butuhkan untuk data loading
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

"""##Mounting google drive karena datasets berada di akun google drive"""

from google.colab import drive
drive.mount('/content/gdrive')

houses =pd.read_csv("gdrive/MyDrive/datasets/USA_Housing.csv")
houses.head()

"""Drop kolom address karena pengaruh alamat terhadap variable y (price) kita sudah di wakili dengan feature Avg. Area Income"""

houses.drop(["Address"],axis=1, inplace=True)

"""Melihat dimensi data frame hasil dibawah ini berarti dataframe memiliki 5000 rows dan 7 kolom"""

houses.shape

"""Rename Kolom supaya lebih mudah dibaca"""

houses.rename(columns={'Avg. Area Income': 'area_income', 
                   'Avg. Area House Age': 'house_age',
                   'Avg. Area Number of Rooms':'numberof_room',
                   'Avg. Area Number of Bedrooms':'numberof_bedrooms',
                   'Area Population':'population'}, inplace=True)
houses.columns

"""Melihat type pada fitur fitur yang kita miliki"""

houses.info()

"""Fiture number of room dan number of bedroom menunjukan jumlah ruangan yang di miliki di masing masing rumah, sangat aneh jika fitur tersebut bertipe float, maka saya akan konversi kedalam integer"""

houses[["numberof_room","numberof_bedrooms"]] = houses[["numberof_room","numberof_bedrooms"]].astype(int)

houses.describe()

"""indentifikasi null dan na value (missing value)"""

houses.isnull().sum()

houses.isna().sum()

houses.columns

"""Menangani Outliers"""

import seaborn as sns
sns.boxplot(x=houses["area_income"])

sns.boxplot(x=houses["house_age"])

sns.boxplot(x=houses["numberof_room"])

sns.boxplot(x=houses["numberof_bedrooms"])

sns.boxplot(x=houses["population"])

"""Dari grafis di atas memberi tahu kita bahwa semua fitur numerik yang kita punya memiliki outliers, mari kita tangani menggunakan methode IQR"""

Q1 = houses.quantile(0.25)
Q3 = houses.quantile(0.75)
IQR = Q3 - Q1

houses = houses[~((houses<(Q1-1.5*IQR))|(houses>(Q3+1.5*IQR))).any(axis=1)]
houses.shape

"""Melihat rumah yang memiliki berapa ruangan yang paling banyak di negara USA"""

count = houses["numberof_room"].value_counts()
count.plot(kind="bar", title="number of room")

count = houses["numberof_bedrooms"].value_counts()
count.plot(kind="bar", title="number of bedroom")

houses["Price"] = houses["Price"]//10000

houses["Price"].plot(kind="hist",bins=50, figsize=(10,5), title="Price")
plt.show()

cat_features = houses[["numberof_room","numberof_bedrooms"]].columns.to_list()
 
for col in cat_features:
  sns.catplot(x=col, y="Price", kind="bar", dodge=False, height = 4, aspect = 3,  data=houses, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

plt.figure(figsize=(10, 8))
correlation_matrix = houses.corr().round(2)
 
# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Table heatmap di atas menunujkan korelasi fitur price sangat tinggi dengan fitur Avg. Area Income dan korelasi paling rendah dengan fitur Avg.Area Number of Bedrooms namun korelasi itu tidak terlalu rendah sehingga kita tidak perlu rop fitur tersebut"""

houses.head()

"""Memisahkan fitur X dan y dan membagi dengan ratio data test 20% dari total data"""

from sklearn.model_selection import train_test_split
 
X = houses.drop(["Price"],axis =1)
y = houses["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

X_train.shape

X_test.shape

X.shape

X_train.info()

from sklearn.preprocessing import MinMaxScaler
 
numerical_features = X_train.select_dtypes(include=["float64", "int64"]).columns
scaler = MinMaxScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import AdaBoostRegressor

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

def grid_search_model(X,y):
    algos = {
        'knn': {
            'model': KNeighborsRegressor(),
            'params': {
                'n_neighbors': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],
            }
        },
        'boosting': {
            'model': AdaBoostRegressor(),
            'params': {
                'learning_rate' : [0.1, 0.05, 0.01, 0.05, 0.001],
                'n_estimators': [25, 50, 75, 100],
                'random_state': [11, 33, 55, 77]
            }
        },
        'random_forest': {
            'model': RandomForestRegressor(),
            'params': {
                'n_estimators': [25, 50, 75, 100],
                'max_depth' : [8, 16, 32, 64],
                'random_state': [11, 33, 55, 77],
            }
        }
        
    }

    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

grid_search_model(X,y)

X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

from sklearn.metrics import mean_squared_error
 
knn = KNeighborsRegressor(n_neighbors=15)
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

# buat model prediksi
RF = RandomForestRegressor(n_estimators=100, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

boosting = AdaBoostRegressor(learning_rate=0.1, random_state=11, n_estimators=100)                             
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""Membuat table untuk masing masing score MSE pada setiap model machine learning yang kita gunakan"""

mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e2
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e2
 
# Panggil mse
mse

knn_accuracy = knn.score(X_test, y_test)*100
rf_accuracy = RF.score(X_test, y_test)*100
boosting_accuracy = boosting.score(X_test, y_test)*100

list_evaluasi = [[knn_accuracy],
            [rf_accuracy],
            [boosting_accuracy]]
evaluasi = pd.DataFrame(list_evaluasi,
                        columns=['Accuracy (%)'],
                        index=['K-Nearest Neighbor', 'Random Forest', 'Boosting'])
evaluasi

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

prediksi = X_test.iloc[:7].copy()
pred_dict = {'y_true':y_test[:7]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)

RF.score(X_test,y_test)